#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <stdbool.h>
#include <float.h>

// ================== Layer: fc1 ================== //
// Transposed weights for layer: fc1 @5x10;
float fc1_weight_transposed[50] = {0.15283203, 0.25463867, 0.34301758, 0.28588867, -0.20971680, -0.42968750, -0.30737305, 0.38378906, -0.26318359, 0.30957031, 0.28906250, -0.26977539, -0.14660645, 0.07574463, -0.24914551, -0.36181641, 0.00963593, -0.15649414, -0.25610352, -0.19079590, 0.02343750, 0.17248535, 0.13793945, -0.40307617, 0.05535889, 0.06951904, -0.02154541, 0.07397461, 0.38134766, 0.05462646, -0.11083984, -0.42846680, -0.15295410, -0.39526367, -0.42602539, -0.02166748, 0.11437988, 0.18249512, -0.16467285, 0.02673340, 0.26611328, -0.29760742, -0.18127441, 0.40380859, -0.32080078, -0.08496094, 0.22961426, 0.36230469, -0.11279297, 0.28857422};
// Biases for layer: fc1 @10;
float fc1_bias_transposed[10] = {-0.02276611, -0.42529297, -0.24780273, -0.15368652, -0.15527344, -0.22460938, 0.32690430, -0.40258789, 0.32055664, 0.32861328};

// ================== Relu: relu1 ================== //
// Relu for layer: relu1;

// ================== Layer: fc2 ================== //
// Transposed weights for layer: fc2 @10x20;
float fc2_weight_transposed[200] = {0.25561523, 0.27587891, -0.26733398, 0.07397461, 0.10284424, 0.22009277, 0.22766113, -0.27563477, 0.13256836, -0.07952881, 0.19104004, 0.29174805, -0.18041992, -0.31152344, -0.04132080, -0.14648438, -0.20983887, 0.01529694, 0.16101074, -0.28417969, -0.19311523, 0.21484375, -0.04217529, 0.09936523, 0.15051270, -0.20214844, -0.24487305, 0.09204102, -0.11059570, -0.18188477, 0.20825195, 0.29394531, 0.05401611, -0.23217773, 0.00240898, 0.04977417, -0.00650787, 0.13000488, 0.25390625, 0.19592285, 0.01960754, -0.30151367, 0.14233398, 0.02926636, 0.19079590, 0.11401367, 0.17773438, -0.15100098, 0.01313782, -0.09130859, 0.27563477, 0.17541504, -0.17236328, 0.23583984, -0.21142578, -0.03152466, -0.05865479, -0.20104980, 0.12341309, 0.29565430, -0.05899048, 0.00519943, 0.13500977, 0.12573242, -0.03704834, 0.24841309, 0.05560303, -0.09857178, -0.10070801, 0.04428101, -0.12707520, -0.23022461, -0.30688477, 0.26220703, -0.00567245, 0.15856934, -0.07208252, -0.27026367, -0.05032349, -0.20593262, 0.19006348, -0.19812012, -0.17749023, -0.29492188, 0.13073730, 0.06069946, -0.28051758, 0.11920166, 0.14001465, 0.03417969, 0.04089355, -0.03976440, 0.27319336, -0.02590942, 0.01266479, 0.26635742, -0.25805664, 0.30273438, -0.07623291, -0.17175293, -0.01977539, -0.24890137, 0.18505859, -0.30688477, -0.19897461, 0.14575195, 0.19274902, 0.23632812, -0.16687012, 0.31445312, -0.06964111, 0.28100586, -0.24035645, 0.27563477, -0.02479553, 0.19287109, -0.25756836, 0.06958008, -0.24987793, 0.22692871, 0.13427734, 0.23815918, 0.22119141, -0.17565918, 0.29223633, -0.02726746, 0.29174805, 0.13098145, 0.00455093, -0.15515137, -0.14282227, -0.29345703, 0.19702148, -0.15148926, 0.07965088, 0.22558594, 0.24743652, -0.09777832, 0.15795898, -0.08630371, -0.09234619, 0.12268066, 0.13757324, 0.04769897, -0.21350098, 0.26855469, 0.09545898, -0.00820160, 0.02584839, -0.25659180, 0.18481445, -0.15332031, 0.23046875, -0.02430725, 0.19616699, -0.26123047, -0.10290527, 0.03518677, 0.17150879, 0.18139648, -0.16906738, -0.13952637, -0.21044922, -0.18041992, 0.16845703, 0.12298584, -0.26708984, 0.17956543, -0.05252075, -0.23657227, 0.30004883, 0.05871582, 0.24987793, 0.07916260, -0.12854004, 0.12103271, 0.07952881, 0.08294678, -0.27075195, -0.11859131, 0.17700195, -0.24877930, 0.21630859, -0.30639648, -0.07659912, -0.15563965, 0.13403320, -0.14379883, 0.04934692, -0.23901367, 0.24816895, 0.05380249, -0.27172852, -0.12817383, -0.09802246, -0.20141602, 0.29418945, -0.17810059, -0.02629089, 0.28198242};
// Biases for layer: fc2 @20;
float fc2_bias_transposed[20] = {-0.26464844, 0.11975098, -0.16906738, 0.31347656, 0.10186768, -0.19287109, 0.15234375, -0.05184937, -0.06311035, 0.30908203, -0.02938843, -0.22497559, 0.03036499, -0.08221436, 0.20471191, 0.06359863, 0.20837402, 0.22326660, 0.17309570, 0.08221436};

// ================== Sigmoid: sigmoid ================== //
// Sigmoid for layer: sigmoid;

// ================== Layer: fc5 ================== //
// Transposed weights for layer: fc5 @20x3;
float fc5_weight_transposed[60] = {-0.06076050, 0.04620361, 0.22326660, 0.06347656, -0.08148193, 0.09631348, 0.21557617, 0.01976013, -0.18041992, -0.16296387, -0.19897461, -0.20983887, -0.21582031, -0.19116211, 0.21130371, -0.22216797, -0.04663086, -0.00136185, -0.14404297, -0.03836060, -0.11657715, 0.05978394, -0.05541992, -0.13891602, 0.12091064, 0.03021240, -0.05337524, -0.11743164, -0.00302696, 0.06933594, 0.00019133, 0.09820557, 0.06082153, -0.20739746, -0.16259766, 0.05145264, -0.04541016, -0.11993408, 0.21765137, -0.15917969, -0.06167603, -0.13146973, 0.22143555, 0.00086689, 0.15295410, 0.13171387, -0.10833740, 0.20019531, 0.12695312, 0.04971313, -0.11169434, 0.13366699, 0.05770874, 0.19714355, 0.21044922, -0.19384766, -0.02558899, 0.07916260, -0.13415527, 0.14050293};
// Biases for layer: fc5 @3;
float fc5_bias_transposed[3] = {0.20922852, 0.17517090, 0.11419678};

// ================== Tanh: tanh ================== //
// Tanh for layer: tanh;

// ================== Layer: fc6 ================== //
// Transposed weights for layer: fc6 @3x2;
float fc6_weight_transposed[6] = {-0.56640625, -0.32934570, 0.00235367, 0.02368164, -0.34179688, 0.17309570};
// Biases for layer: fc6 @2;
float fc6_bias_transposed[2] = {0.55029297, -0.31958008};

// ================== SoftMax: softmax2 ================== //
// SoftMax for layer: softmax2;

float *Linear(int batch_size, int input_size, int output_size, float *input, float *weight_transposed, float *bias)
{
    int i, j, k;

    // 为结果矩阵分配内存
    float *result = (float *)malloc(batch_size * output_size * sizeof(float));
    if (result == NULL)
    {
        printf("Error: Memory allocation failed.\n");
        return NULL;
    }

    // 初始化并执行矩阵乘法
    for (i = 0; i < batch_size; i++)
    {
        for (j = 0; j < output_size; j++)
        {
            result[i * output_size + j] = 0; // 初始化元素为0
            for (k = 0; k < input_size; k++)
            {
                result[i * output_size + j] += input[i * input_size + k] * weight_transposed[k * output_size + j];
            }
            result[i * output_size + j] += bias[j]; // 将偏置项加到结果中
        }
    }

    // 返回结果矩阵
    return result;
}
void Relu(int batch_size, int elements_length, float *input)
{
    for (int i = 0; i < batch_size * elements_length; i++)
    {
        if (input[i] < 0)
        {
            input[i] = 0;
        }
    }
}
void sigmoid(int batch_size, int elements_length, float *input)
{
    // 遍历输入数组的每个元素
    for (int i = 0; i < batch_size; i++)
    {
        for (int j = 0; j < elements_length; j++)
        {
            int index = i * elements_length + j;
            // 计算 Sigmoid 值并更新输入数组
            input[index] = 1.0 / (1.0 + exp(-input[index]));
        }
    }
}
void Tanh(int batch_size, int elements_length, float *input)
{
    // 遍历输入数组的每个元素
    for (int i = 0; i < batch_size; i++)
    {
        for (int j = 0; j < elements_length; j++)
        {
            int index = i * elements_length + j;
            // 计算 tanh 值并更新输入数组
            input[index] = tanh(input[index]);
        }
    }
}
void SoftMax(int batch_size, int elements_length, float *input)
{
    float max_value, sum_exp;

    // 逐行计算softmax
    for (int i = 0; i < batch_size; i++)
    {
        // 找到该行的最大值
        max_value = input[i * elements_length];
        for (int j = 1; j < elements_length; j++)
        {
            if (input[i * elements_length + j] > max_value)
            {
                max_value = input[i * elements_length + j];
            }
        }

        // 计算该行的指数和
        sum_exp = 0.0f;
        for (int j = 0; j < elements_length; j++)
        {
            input[i * elements_length + j] = exp(input[i * elements_length + j] - max_value);
            sum_exp += input[i * elements_length + j];
        }

        // 归一化得到softmax输出
        for (int j = 0; j < elements_length; j++)
        {
            input[i * elements_length + j] /= sum_exp;
        }
    }
}

void forward(float input[], float output[]){
	float* result_0=(float*)malloc(sizeof(float)*5);
for (int i = 0; i < 5; i++) { result_0[i] = input[i]; }
// fc1_layer
float* result_1 = Linear(1,5,10,result_0,fc1_weight_transposed,fc1_bias_transposed);
free(result_0);
// relu1_relu
Relu(1,10,result_1);
// fc2_layer
float* result_2 = Linear(1,10,20,result_1,fc2_weight_transposed,fc2_bias_transposed);
free(result_1);
// sigmoid_layer
sigmoid(1,20,result_2);
// fc5_layer
float* result_3 = Linear(1,20,3,result_2,fc5_weight_transposed,fc5_bias_transposed);
free(result_2);
// tanh_layer
Tanh(1,3,result_3);
// fc6_layer
float* result_4 = Linear(1,3,2,result_3,fc6_weight_transposed,fc6_bias_transposed);
free(result_3);
// softmax2_layer
SoftMax(1,2,result_4);
for (int i = 0; i < 2; i++) { output[i] = result_4[i]; }
	free(result_4);
}
int main(){
float input[5] = { 0.9134092330932617,-0.7428250312805176,-0.353948175907135,0.7984378933906555,-1.138546109199524 };
float output[2];
forward(input, output);
for (int i = 0; i < 2; i++){ printf("%f  ", output[i]); 
 }
return 0;
}